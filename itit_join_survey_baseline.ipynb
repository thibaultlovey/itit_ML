{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_clean = pd.read_pickle('data_clean/baseline_clean.pkl')\n",
    "survey_clean = pd.read_pickle('data_clean/survey_clean.pkl')\n",
    "follow_survey = pd.read_pickle('data_clean/follow_survey.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "itit_df = pd.merge(baseline_clean, survey_clean, on=['trip_id', 'user_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_travel_date_year: Adjusts the year in travel_date to match the year of baseline_date if it falls outside the range [2022, 2025]\n",
    "itit_df['travel_date'] = itit_df.apply(lambda row: row['travel_date'].replace(year=row['baseline_date'].year) if row['travel_date'].year < 2022 or row['travel_date'].year > 2025 else row['travel_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing surveys\n",
    "def add_missing_surveys(df):\n",
    "    survey_list = []\n",
    "    \n",
    "    for (trip_id, user_id), group in df.groupby(['trip_id', 'user_id']):\n",
    "        travel_date = group['travel_date'].iloc[0]\n",
    "        duration = int(group['travel_duration'].iloc[0])\n",
    "        \n",
    "        # Create a DataFrame with all expected survey dates\n",
    "        all_days = pd.date_range(start=travel_date, periods=duration, freq='D')\n",
    "        existing_days = group['finished'].dt.date.unique()\n",
    "        \n",
    "        # Filter out days that already have surveys\n",
    "        missing_days = [day for day in all_days if day.date() not in existing_days]\n",
    "        \n",
    "        # Create DataFrame for missing surveys\n",
    "        if missing_days:\n",
    "            missing_surveys = pd.DataFrame({\n",
    "                'trip_id': trip_id,\n",
    "                'user_id': user_id,\n",
    "                'travel_date': travel_date,\n",
    "                'finished': pd.to_datetime(missing_days),\n",
    "                'travel_duration': duration\n",
    "            })\n",
    "            \n",
    "            # Copy constant baseline variables\n",
    "            baseline_vars = ['baseline_date', 'age', 'gender', 'country_iso2c', 'country_clean', 'continent_clean', \n",
    "                             'health_chronic', 'latitude', 'longitude', 'smoking_status', 'travel_purpose', 'trip_number']\n",
    "            for var in baseline_vars:\n",
    "                missing_surveys[var] = group[var].iloc[0]\n",
    "                \n",
    "            # Append missing surveys to the list\n",
    "            survey_list.append(missing_surveys)\n",
    "        \n",
    "        # Append existing surveys to the list\n",
    "        survey_list.append(group)\n",
    "    \n",
    "    # Concatenate all DataFrames and sort\n",
    "    df_concat = pd.concat(survey_list).sort_values(by=['trip_id', 'user_id', 'finished']).reset_index(drop=True)\n",
    "\n",
    "    # Ensure finished column is properly filled\n",
    "    df_concat['finished'] = df_concat['finished'].ffill().bfill()\n",
    "\n",
    "    # Ensure no duplicate rows by dropping the extra row caused by the duration miscalculation\n",
    "    df_concat = df_concat.drop_duplicates(subset=['trip_id', 'user_id', 'finished'])\n",
    "    \n",
    "    return df_concat\n",
    "\n",
    "itit_df = add_missing_surveys(itit_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out surveys that occurred before the departure date\n",
    "itit_df = itit_df[itit_df['finished'] >= itit_df['travel_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign Travel Duration Based on Trip ID Count\n",
    "itit_df['travel_duration'] = itit_df.groupby('trip_id')['trip_id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing follow up survey with allocating feedback 1 or 2 correctly\n",
    "finish_trip_df = (itit_df.loc[itit_df.groupby('trip_id')['finished'].idxmax(), ['user_id', 'trip_id', 'finished']]\n",
    "                      .rename(columns={'finished': 'finish_trip_date'}))\n",
    "follow_survey_corr = pd.merge(\n",
    "    follow_survey, \n",
    "    finish_trip_df, \n",
    "    how='left', \n",
    "    on=['trip_id', 'user_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4x/g578bp9s1bs7z_0185h2n1wr0000gn/T/ipykernel_95015/527991706.py:44: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  follow_survey_corr.groupby('trip_id', group_keys=False).apply(classify_follow_ups)\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'finished' and 'finish_trip_date' columns are in datetime format\n",
    "follow_survey_corr['finished'] = pd.to_datetime(follow_survey_corr['finished'])\n",
    "follow_survey_corr['finish_trip_date'] = pd.to_datetime(follow_survey_corr['finish_trip_date'])\n",
    "\n",
    "# Define the function to classify follow-up surveys\n",
    "def classify_follow_ups(group):\n",
    "    follow_ups = []\n",
    "    seen_follow_up_1 = False\n",
    "    seen_follow_up_2 = False\n",
    "    \n",
    "    for idx, row in group.iterrows():\n",
    "        days_since_trip = (row['finished'] - row['finish_trip_date']).days\n",
    "        \n",
    "        # Check if it's Follow-up 1 or Follow-up 2\n",
    "        if 4 <= days_since_trip <= 21:  # Within 7-day window (flexible)\n",
    "            if not seen_follow_up_1:\n",
    "                follow_ups.append(\"Follow-up 1\")\n",
    "                seen_follow_up_1 = True\n",
    "            else:\n",
    "                follow_ups.append(\"Duplicate\")\n",
    "        elif 21 < days_since_trip <= 45:  # Within 30-day window (flexible)\n",
    "            if not seen_follow_up_2:\n",
    "                follow_ups.append(\"Follow-up 2\")\n",
    "                seen_follow_up_2 = True\n",
    "            else:\n",
    "                follow_ups.append(\"Duplicate\")\n",
    "        else:\n",
    "            # Assign based on closest range if only one survey exists\n",
    "            if not seen_follow_up_1 and not seen_follow_up_2:\n",
    "                if abs(days_since_trip - 7) < abs(days_since_trip - 30):\n",
    "                    follow_ups.append(\"Follow-up 1\")\n",
    "                    seen_follow_up_1 = True\n",
    "                else:\n",
    "                    follow_ups.append(\"Follow-up 2\")\n",
    "                    seen_follow_up_2 = True\n",
    "            else:\n",
    "                follow_ups.append(\"Duplicate\")\n",
    "    \n",
    "    return pd.Series(follow_ups, index=group.index)\n",
    "\n",
    "# Group by trip_id and classify follow-up surveys\n",
    "follow_survey_corr = follow_survey_corr.sort_values(by=['trip_id', 'finished'])\n",
    "follow_survey_corr['follow_up_survey'] = (\n",
    "    follow_survey_corr.groupby('trip_id', group_keys=False).apply(classify_follow_ups)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the aggregation logic for the desired columns\n",
    "def aggregate_column(column):\n",
    "    # Drop NAs, keep unique values, and join them with a comma\n",
    "    return ', '.join(column.dropna().astype(str).unique())\n",
    "\n",
    "# Create a new DataFrame with aggregated values per trip_id\n",
    "aggregated_values = (\n",
    "    follow_survey_corr.groupby(\"trip_id\", as_index=False)\n",
    "    .agg({\n",
    "        \"diagnosis\": aggregate_column,\n",
    "        \"diagnosis_date\": lambda x: ', '.join(x.dropna().astype(str).unique()),\n",
    "        \"self_treament\": aggregate_column\n",
    "    })\n",
    ")\n",
    "\n",
    "# Merge the aggregated values back into the original dataset\n",
    "follow_survey_corr = follow_survey_corr.merge(aggregated_values, on=\"trip_id\", suffixes=('', '_filled'))\n",
    "\n",
    "# Fill the original columns with the filled values\n",
    "follow_survey_corr[\"diagnosis\"] = follow_survey_corr[\"diagnosis_filled\"]\n",
    "follow_survey_corr[\"diagnosis_date\"] = follow_survey_corr[\"diagnosis_date_filled\"]\n",
    "follow_survey_corr[\"self_treament\"] = follow_survey_corr[\"self_treament_filled\"]\n",
    "\n",
    "# Drop the intermediate filled columns\n",
    "follow_survey_corr = follow_survey_corr.drop(columns=[\"diagnosis_filled\", \"diagnosis_date_filled\", \"self_treament_filled\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_survey_corr.to_csv('data_clean/follow_survey_corr.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out duplicates\n",
    "follow_survey_corr = follow_survey_corr[follow_survey_corr['follow_up_survey'] != \"Duplicate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data to wide format\n",
    "follow_survey_wide = follow_survey_corr.pivot(\n",
    "    index=['trip_id', 'user_id'],\n",
    "    columns='follow_up_survey'\n",
    ")\n",
    "\n",
    "# Flatten the multi-index columns and rename them\n",
    "follow_survey_wide.columns = [\n",
    "    f\"{str(col[1]).replace(' ', '_').lower()}_{col[0]}\" \n",
    "    for col in follow_survey_wide.columns\n",
    "]\n",
    "\n",
    "# Reset the index to make 'trip_id' and 'user_id' columns\n",
    "follow_survey_wide = follow_survey_wide.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "itit_df_clean = pd.merge(\n",
    "    itit_df, \n",
    "    follow_survey_wide, \n",
    "    how='left', \n",
    "    on=['trip_id', 'user_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a consistent 10-character alphanumeric hash\n",
    "def generate_hash(value):\n",
    "    unique_value = f\"{value}\"  # Ensure it's a string\n",
    "    hashed_value = abs(hash(unique_value))  # Get the hash and ensure it's positive\n",
    "    return str(hashed_value)[:15]  # Take the first 10 characters\n",
    "\n",
    "# Apply the hash function to the columns\n",
    "itit_df_clean['trip_id'] = itit_df_clean['trip_id'].astype('category').cat.codes.apply(lambda x: generate_hash(x))\n",
    "itit_df_clean['user_id'] = itit_df_clean['user_id'].astype('category').cat.codes.apply(lambda x: generate_hash(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csv\n",
    "itit_df_clean.to_csv('data_clean/itit_df_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "720217501014479    545\n",
       "556474885282632    455\n",
       "277587465624496    365\n",
       "801577621909135    365\n",
       "413646352847780    365\n",
       "                  ... \n",
       "384399750461871      2\n",
       "463925000141011      1\n",
       "695902468025654      1\n",
       "442562437470473      1\n",
       "510394461004765      1\n",
       "Name: count, Length: 1432, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itit_df_clean.user_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49778 entries, 0 to 49777\n",
      "Data columns (total 156 columns):\n",
      " #    Column                             Dtype         \n",
      "---   ------                             -----         \n",
      " 0    trip_id                            object        \n",
      " 1    user_id                            object        \n",
      " 2    travel_date                        datetime64[ns]\n",
      " 3    finished                           datetime64[ns]\n",
      " 4    travel_duration                    int64         \n",
      " 5    baseline_date                      datetime64[ns]\n",
      " 6    age                                float64       \n",
      " 7    gender                             object        \n",
      " 8    country_iso2c                      object        \n",
      " 9    country_clean                      object        \n",
      " 10   continent_clean                    object        \n",
      " 11   health_chronic                     object        \n",
      " 12   latitude                           float64       \n",
      " 13   longitude                          float64       \n",
      " 14   smoking_status                     object        \n",
      " 15   travel_purpose                     object        \n",
      " 16   trip_number                        object        \n",
      " 17   gastro_any                         object        \n",
      " 18   nausea                             object        \n",
      " 19   vomiting                           object        \n",
      " 20   stomach_pain                       object        \n",
      " 21   diarrhea                           object        \n",
      " 22   constipation                       object        \n",
      " 23   respi_any                          object        \n",
      " 24   cough                              object        \n",
      " 25   sore_throat                        object        \n",
      " 26   runny_nose                         object        \n",
      " 27   out_of_breath_resting              object        \n",
      " 28   out_of_breath_running              object        \n",
      " 29   skin_any                           object        \n",
      " 30   rash                               object        \n",
      " 31   itchy_insect_bite                  object        \n",
      " 32   itchy_other                        object        \n",
      " 33   sunburn                            object        \n",
      " 34   itchy_red_eyes                     object        \n",
      " 35   body_any                           object        \n",
      " 36   fever                              object        \n",
      " 37   dizziness                          object        \n",
      " 38   ear_ache                           object        \n",
      " 39   headache                           object        \n",
      " 40   pain_eyes                          object        \n",
      " 41   musle_pain                         object        \n",
      " 42   aching_limbs                       object        \n",
      " 43   body_other                         object        \n",
      " 44   joint_any                          object        \n",
      " 45   pain_joint                         object        \n",
      " 46   swelling_joint                     object        \n",
      " 47   location_swelling                  object        \n",
      " 48   impact                             object        \n",
      " 49   rating_day                         object        \n",
      " 50   survey_latitude                    float64       \n",
      " 51   survey_longitude                   float64       \n",
      " 52   clouds                             float64       \n",
      " 53   dew_point                          float64       \n",
      " 54   feels_like                         float64       \n",
      " 55   humidity                           float64       \n",
      " 56   pressure                           float64       \n",
      " 57   rain_1h                            float64       \n",
      " 58   snow_1h                            float64       \n",
      " 59   sunrise                            float64       \n",
      " 60   sunset                             float64       \n",
      " 61   temp                               float64       \n",
      " 62   uvi                                float64       \n",
      " 63   visibility                         float64       \n",
      " 64   main_weather                       object        \n",
      " 65   description_weather                object        \n",
      " 66   air_quality_components_co          float64       \n",
      " 67   air_quality_components_nh_3        float64       \n",
      " 68   air_quality_components_no          float64       \n",
      " 69   air_quality_components_no_2        float64       \n",
      " 70   air_quality_components_o_3         float64       \n",
      " 71   air_quality_components_pm_10       float64       \n",
      " 72   air_quality_components_pm_2_5      float64       \n",
      " 73   air_quality_components_so_2        float64       \n",
      " 74   wind_deg                           float64       \n",
      " 75   wind_gust                          float64       \n",
      " 76   wind_speed                         float64       \n",
      " 77   air_quality_main                   object        \n",
      " 78   follow-up_1_finished               datetime64[ns]\n",
      " 79   follow-up_2_finished               datetime64[ns]\n",
      " 80   follow-up_1_gastro_any             object        \n",
      " 81   follow-up_2_gastro_any             object        \n",
      " 82   follow-up_1_nausea                 object        \n",
      " 83   follow-up_2_nausea                 object        \n",
      " 84   follow-up_1_vomiting               object        \n",
      " 85   follow-up_2_vomiting               object        \n",
      " 86   follow-up_1_stomach_pain           object        \n",
      " 87   follow-up_2_stomach_pain           object        \n",
      " 88   follow-up_1_diarrhea               object        \n",
      " 89   follow-up_2_diarrhea               object        \n",
      " 90   follow-up_1_constipation           object        \n",
      " 91   follow-up_2_constipation           object        \n",
      " 92   follow-up_1_respi_any              object        \n",
      " 93   follow-up_2_respi_any              object        \n",
      " 94   follow-up_1_cough                  object        \n",
      " 95   follow-up_2_cough                  object        \n",
      " 96   follow-up_1_sore_throat            object        \n",
      " 97   follow-up_2_sore_throat            object        \n",
      " 98   follow-up_1_runny_nose             object        \n",
      " 99   follow-up_2_runny_nose             object        \n",
      " 100  follow-up_1_out_of_breath_resting  object        \n",
      " 101  follow-up_2_out_of_breath_resting  object        \n",
      " 102  follow-up_1_out_of_breath_running  object        \n",
      " 103  follow-up_2_out_of_breath_running  object        \n",
      " 104  follow-up_1_skin_any               object        \n",
      " 105  follow-up_2_skin_any               object        \n",
      " 106  follow-up_1_rash                   object        \n",
      " 107  follow-up_2_rash                   object        \n",
      " 108  follow-up_1_itchy_insect_bite      object        \n",
      " 109  follow-up_2_itchy_insect_bite      object        \n",
      " 110  follow-up_1_itchy_other            object        \n",
      " 111  follow-up_2_itchy_other            object        \n",
      " 112  follow-up_1_sunburn                object        \n",
      " 113  follow-up_2_sunburn                object        \n",
      " 114  follow-up_1_itchy_red_eyes         object        \n",
      " 115  follow-up_2_itchy_red_eyes         object        \n",
      " 116  follow-up_1_body_any               object        \n",
      " 117  follow-up_2_body_any               object        \n",
      " 118  follow-up_1_fever                  object        \n",
      " 119  follow-up_2_fever                  object        \n",
      " 120  follow-up_1_dizziness              object        \n",
      " 121  follow-up_2_dizziness              object        \n",
      " 122  follow-up_1_ear_ache               object        \n",
      " 123  follow-up_2_ear_ache               object        \n",
      " 124  follow-up_1_headache               object        \n",
      " 125  follow-up_2_headache               object        \n",
      " 126  follow-up_1_pain_eyes              object        \n",
      " 127  follow-up_2_pain_eyes              object        \n",
      " 128  follow-up_1_musle_pain             object        \n",
      " 129  follow-up_2_musle_pain             object        \n",
      " 130  follow-up_1_aching_limbs           object        \n",
      " 131  follow-up_2_aching_limbs           object        \n",
      " 132  follow-up_1_body_other             object        \n",
      " 133  follow-up_2_body_other             object        \n",
      " 134  follow-up_1_joint_any              object        \n",
      " 135  follow-up_2_joint_any              object        \n",
      " 136  follow-up_1_pain_joint             object        \n",
      " 137  follow-up_2_pain_joint             object        \n",
      " 138  follow-up_1_swelling_joint         object        \n",
      " 139  follow-up_2_swelling_joint         object        \n",
      " 140  follow-up_1_location_swelling      object        \n",
      " 141  follow-up_2_location_swelling      object        \n",
      " 142  follow-up_1_impact                 object        \n",
      " 143  follow-up_2_impact                 object        \n",
      " 144  follow-up_1_rating_day             object        \n",
      " 145  follow-up_2_rating_day             object        \n",
      " 146  follow-up_1_consulted_doctor       object        \n",
      " 147  follow-up_2_consulted_doctor       object        \n",
      " 148  follow-up_1_self_treament          object        \n",
      " 149  follow-up_2_self_treament          object        \n",
      " 150  follow-up_1_diagnosis              object        \n",
      " 151  follow-up_2_diagnosis              object        \n",
      " 152  follow-up_1_diagnosis_date         object        \n",
      " 153  follow-up_2_diagnosis_date         object        \n",
      " 154  follow-up_1_finish_trip_date       datetime64[ns]\n",
      " 155  follow-up_2_finish_trip_date       datetime64[ns]\n",
      "dtypes: datetime64[ns](7), float64(28), int64(1), object(120)\n",
      "memory usage: 59.2+ MB\n"
     ]
    }
   ],
   "source": [
    "itit_df_clean.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only participant that filled at least one survey\n",
    "itit_df_1plus_survey=itit_df.groupby('trip_id').filter(lambda x: x['nausea'].notna().any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filled missing survey based on symptoms length (forward and backward)\n",
    "def fill_columns_forward(data, columns,baselevel, max_repetition=4):\n",
    "    # Loop through each column provided\n",
    "    for column in columns:\n",
    "        # Create a new column to store the filled data\n",
    "        fill_column = f'{column}_filled'\n",
    "        data[fill_column] = data[column].copy()\n",
    "\n",
    "        # Loop through each trip_id\n",
    "        for tid, tdata in data.groupby('trip_id'):\n",
    "            # Sort the grouped data by 'finished' for chronological order\n",
    "            tdata = tdata.sort_values(by='finished')\n",
    "\n",
    "            # We need to work with indices because we look back at previous rows\n",
    "            indices = tdata.index\n",
    "            for i in range(len(indices)):\n",
    "                idx = indices[i]\n",
    "                current_value = tdata.at[idx, column]\n",
    "\n",
    "                if pd.isna(current_value):\n",
    "                    # Look back to count repetitions of the last valid entry if it exists\n",
    "                    last_valid = None\n",
    "                    count_back = 0\n",
    "                    for j in range(1, min(max_repetition, i) + 1):\n",
    "                        back_idx = indices[i - j]\n",
    "                        back_value = tdata.at[back_idx, column]\n",
    "                        if not pd.isna(back_value):\n",
    "                            if last_valid is None:\n",
    "                                last_valid = back_value\n",
    "                            if back_value == last_valid:\n",
    "                                count_back += 1\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "                    # If the count is less than max_repetition and last_valid is not 'none', fill with last_valid\n",
    "                    if count_back < max_repetition and last_valid != baselevel[column]:\n",
    "                        data.loc[idx, fill_column] = last_valid\n",
    "                else:\n",
    "                    # Update the filled column with current value\n",
    "                    data.loc[idx, fill_column] = current_value\n",
    "\n",
    "    return data\n",
    "\n",
    "def fill_columns_backward(data, columns,baselevel, max_repetition=4):\n",
    "    # Loop through each column provided\n",
    "    for column in columns:\n",
    "        fill_column = f'{column}_filled'\n",
    "        \n",
    "        # Ensure the 'nausea_filled' column exists, copying the original data if not\n",
    "        if fill_column not in data.columns:\n",
    "            data[fill_column] = data[column].copy()\n",
    "\n",
    "        # Loop through each trip_id\n",
    "        for tid, tdata in data.groupby('trip_id'):\n",
    "            # Sort the grouped data by 'finished' in descending order for backward filling\n",
    "            tdata = tdata.sort_values(by='finished', ascending=False)\n",
    "\n",
    "            # We need to work with indices because we look back at previous rows\n",
    "            indices = tdata.index\n",
    "            last_valid = None\n",
    "            count_back = 0\n",
    "            continuous_na = False  # Track if we are in a continuous NA segment\n",
    "\n",
    "            for i in range(len(indices)):\n",
    "                idx = indices[i]\n",
    "                current_value = tdata.at[idx, fill_column]\n",
    "\n",
    "                if pd.isna(current_value):\n",
    "                    if continuous_na:\n",
    "                        # If we are in a continuous NA segment, reset last_valid\n",
    "                        last_valid = None\n",
    "                        count_back = 0\n",
    "                    elif last_valid is not None and last_valid != baselevel[column] and count_back < max_repetition:\n",
    "                        data.loc[idx, fill_column] = last_valid\n",
    "                        count_back += 1\n",
    "                    else:\n",
    "                        # Reset the tracking variables if conditions to fill are not met\n",
    "                        last_valid = None\n",
    "                        count_back = 0\n",
    "                    continuous_na = True  # Mark that we are in a continuous NA segment\n",
    "                else:\n",
    "                    # Reset continuous NA tracking as we hit a non-NA value\n",
    "                    continuous_na = False\n",
    "                    if current_value == last_valid:\n",
    "                        count_back += 1\n",
    "                    else:\n",
    "                        last_valid = current_value\n",
    "                        count_back = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "columns_to_fill = ['nausea', 'vomiting', 'stomach_pain', 'diarrhea',\n",
    "       'constipation','cough', 'sore_throat', 'runny_nose',\n",
    "       'out_of_breath_resting', 'out_of_breath_running','rash',\n",
    "       'itchy_insect_bite', 'itchy_other', 'sunburn', 'itchy_red_eyes','fever', 'dizziness', 'ear_ache', 'headache', 'pain_eyes',\n",
    "       'musle_pain', 'aching_limbs','pain_joint',\n",
    "       'swelling_joint', 'location_swelling',\n",
    "       'body_other',\n",
    "       'impact',\n",
    "       'rating_day'\n",
    "       ]\n",
    "base_level = {\n",
    "    'nausea': 'none',\n",
    "    'vomiting': 'none',\n",
    "    'stomach_pain': 'none',\n",
    "    'diarrhea': 'none',\n",
    "    'constipation': 'none',\n",
    "    'cough': 'none',\n",
    "    'sore_throat': 'none',\n",
    "    'runny_nose': 'none',\n",
    "    'out_of_breath_resting': 'none',\n",
    "    'out_of_breath_running': 'none',\n",
    "    'rash': 'none',\n",
    "    'itchy_insect_bite': 'none',\n",
    "    'itchy_other': 'none',\n",
    "    'sunburn': 'none',\n",
    "    'itchy_red_eyes': 'none',\n",
    "    'fever': 'none',\n",
    "    'dizziness': 'none',\n",
    "    'ear_ache': 'none',\n",
    "    'headache': 'none',\n",
    "    'pain_eyes': 'none',\n",
    "    'musle_pain': 'none',\n",
    "    'aching_limbs': 'none',\n",
    "    'pain_joint': 'none',\n",
    "    'swelling_joint': 'none',\n",
    "    'location_swelling': 'none',\n",
    "    'body_other': 'No',\n",
    "    'impact':'Did not affect my activities',\n",
    "    'rating_day': 'It was a great day'\n",
    "}\n",
    "\n",
    "itit_filled_df = fill_columns_forward(itit_df_1plus_survey, columns_to_fill, baselevel=base_level)\n",
    "itit_filled_df = fill_columns_backward(itit_filled_df, columns_to_fill, baselevel=base_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the filled column names\n",
    "filled_columns = [f'{column}_filled' for column in columns_to_fill]\n",
    "\n",
    "# Adjust the base level mapping to match the filled column names\n",
    "filled_base_level = {f'{column}_filled': base_level[column] for column in columns_to_fill}\n",
    "\n",
    "# Fill NA values based on the adjusted base level mapping\n",
    "itit_filled_df[filled_columns] = itit_filled_df[filled_columns].fillna(filled_base_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_numeric_columns(data, columns, priority='last'):\n",
    "    for column in columns:\n",
    "        fill_column = f'{column}_filled'\n",
    "        data[fill_column] = data[column].copy()\n",
    "\n",
    "        for tid, tdata in data.groupby('trip_id'):\n",
    "            tdata = tdata.sort_values(by='finished')\n",
    "            indices = tdata.index.tolist()\n",
    "            n = len(indices)\n",
    "\n",
    "            I = 0\n",
    "            while I < n:\n",
    "                idx = indices[I]\n",
    "                if pd.isna(tdata.at[idx, column]):\n",
    "                    start_idx = I\n",
    "                    # Find the segment of missing values\n",
    "                    while I < n and pd.isna(tdata.at[indices[I], column]):\n",
    "                        I += 1\n",
    "                    end_idx = I\n",
    "\n",
    "                    # Get the previous and next valid values\n",
    "                    prev_value = tdata.at[indices[start_idx - 1], column] if start_idx > 0 else None\n",
    "                    next_value = tdata.at[indices[end_idx], column] if end_idx < n else None\n",
    "\n",
    "                    # Fill the missing values based on the priority\n",
    "                    num_missing = end_idx - start_idx\n",
    "                    half_point = (num_missing + 1) // 2\n",
    "\n",
    "                    if prev_value is not None and next_value is not None:\n",
    "                        if priority == 'last':\n",
    "                            for j in range(num_missing):\n",
    "                                fill_idx = indices[start_idx + j]\n",
    "                                if j < half_point:\n",
    "                                    data.loc[fill_idx, fill_column] = prev_value\n",
    "                                else:\n",
    "                                    data.loc[fill_idx, fill_column] = next_value\n",
    "                        elif priority == 'first':\n",
    "                            for j in range(num_missing):\n",
    "                                fill_idx = indices[start_idx + j]\n",
    "                                if j < half_point:\n",
    "                                    data.loc[fill_idx, fill_column] = next_value\n",
    "                                else:\n",
    "                                    data.loc[fill_idx, fill_column] = prev_value\n",
    "                    elif prev_value is not None:\n",
    "                        for j in range(num_missing):\n",
    "                            fill_idx = indices[start_idx + j]\n",
    "                            data.loc[fill_idx, fill_column] = prev_value\n",
    "                    elif next_value is not None:\n",
    "                        for j in range(num_missing):\n",
    "                            fill_idx = indices[start_idx + j]\n",
    "                            data.loc[fill_idx, fill_column] = next_value\n",
    "                else:\n",
    "                    I += 1\n",
    "\n",
    "    return data\n",
    "columns_to_fill_numeric = ['trip_id',\n",
    " 'user_id',\n",
    " 'travel_date',\n",
    " 'finished',\n",
    " 'travel_duration',\n",
    " 'baseline_date',\n",
    " 'age',\n",
    " 'gender',\n",
    " 'country_iso2c',\n",
    " 'country_clean',\n",
    " 'continent_clean',\n",
    " 'health_chronic',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'smoking_status',\n",
    " 'travel_purpose',\n",
    " 'trip_number',\n",
    " 'clouds','survey_latitude',\n",
    " 'survey_longitude',\n",
    " 'clouds',\n",
    " 'dew_point',\n",
    " 'feels_like',\n",
    " 'humidity',\n",
    " 'pressure',\n",
    " 'rain_1h',\n",
    " 'snow_1h',\n",
    " 'sunrise',\n",
    " 'sunset',\n",
    " 'temp',\n",
    " 'uvi',\n",
    " 'visibility',\n",
    " 'main_weather',\n",
    " 'description_weather',\n",
    " 'air_quality_components_co',\n",
    " 'air_quality_components_nh_3',\n",
    " 'air_quality_components_no',\n",
    " 'air_quality_components_no_2',\n",
    " 'air_quality_components_o_3',\n",
    " 'air_quality_components_pm_10',\n",
    " 'air_quality_components_pm_2_5',\n",
    " 'air_quality_components_so_2',\n",
    " 'wind_deg',\n",
    " 'wind_gust',\n",
    " 'wind_speed',\n",
    "  'air_quality_main']\n",
    "itit_filled_df = fill_numeric_columns(itit_filled_df, columns_to_fill_numeric, priority='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove initial columns\n",
    "itit_filled_df = itit_filled_df.drop(columns=columns_to_fill)\n",
    "itit_filled_df = itit_filled_df.drop(columns=columns_to_fill_numeric)\n",
    "itit_filled_df = itit_filled_df.drop(columns=['respi_any','gastro_any','skin_any','body_any','joint_any'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat dataset without gps position\n",
    "filled_columns_numeric = [f'{column}_filled' for column in columns_to_fill_numeric[columns_to_fill_numeric.index('clouds'):columns_to_fill_numeric.index('air_quality_main') + 1]]\n",
    "itit_nogps_filled_df = itit_filled_df.copy()\n",
    "itit_nogps_filled_df = itit_nogps_filled_df.drop(columns=filled_columns_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat dataset with gps position\n",
    "itit_filled_df=itit_filled_df.groupby('trip_id_filled').filter(lambda x: x['air_quality_main_filled'].notna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "itit_filled_df[['rain_1h_filled','snow_1h_filled','wind_gust_filled']]=itit_filled_df[['rain_1h_filled','snow_1h_filled','wind_gust_filled']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "itit_nogps_filled_df.to_pickle('data_clean/itit_nogps_filled_df.pkl')\n",
    "itit_filled_df.to_pickle('data_clean/itit_filled_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22149 entries, 11 to 39345\n",
      "Data columns (total 73 columns):\n",
      " #   Column                                Non-Null Count  Dtype         \n",
      "---  ------                                --------------  -----         \n",
      " 0   nausea_filled                         22149 non-null  object        \n",
      " 1   vomiting_filled                       22149 non-null  object        \n",
      " 2   stomach_pain_filled                   22149 non-null  object        \n",
      " 3   diarrhea_filled                       22149 non-null  object        \n",
      " 4   constipation_filled                   22149 non-null  object        \n",
      " 5   cough_filled                          22149 non-null  object        \n",
      " 6   sore_throat_filled                    22149 non-null  object        \n",
      " 7   runny_nose_filled                     22149 non-null  object        \n",
      " 8   out_of_breath_resting_filled          22149 non-null  object        \n",
      " 9   out_of_breath_running_filled          22149 non-null  object        \n",
      " 10  rash_filled                           22149 non-null  object        \n",
      " 11  itchy_insect_bite_filled              22149 non-null  object        \n",
      " 12  itchy_other_filled                    22149 non-null  object        \n",
      " 13  sunburn_filled                        22149 non-null  object        \n",
      " 14  itchy_red_eyes_filled                 22149 non-null  object        \n",
      " 15  fever_filled                          22149 non-null  object        \n",
      " 16  dizziness_filled                      22149 non-null  object        \n",
      " 17  ear_ache_filled                       22149 non-null  object        \n",
      " 18  headache_filled                       22149 non-null  object        \n",
      " 19  pain_eyes_filled                      22149 non-null  object        \n",
      " 20  musle_pain_filled                     22149 non-null  object        \n",
      " 21  aching_limbs_filled                   22149 non-null  object        \n",
      " 22  pain_joint_filled                     22149 non-null  object        \n",
      " 23  swelling_joint_filled                 22149 non-null  object        \n",
      " 24  location_swelling_filled              22149 non-null  object        \n",
      " 25  body_other_filled                     22149 non-null  object        \n",
      " 26  impact_filled                         22149 non-null  object        \n",
      " 27  rating_day_filled                     22149 non-null  object        \n",
      " 28  trip_id_filled                        22149 non-null  object        \n",
      " 29  user_id_filled                        22149 non-null  object        \n",
      " 30  travel_date_filled                    22149 non-null  datetime64[ns]\n",
      " 31  finished_filled                       22149 non-null  datetime64[ns]\n",
      " 32  travel_duration_filled                22149 non-null  float64       \n",
      " 33  baseline_date_filled                  22149 non-null  datetime64[ns]\n",
      " 34  age_filled                            22149 non-null  float64       \n",
      " 35  gender_filled                         22066 non-null  object        \n",
      " 36  country_iso2c_filled                  22149 non-null  object        \n",
      " 37  country_clean_filled                  16111 non-null  object        \n",
      " 38  continent_clean_filled                22128 non-null  object        \n",
      " 39  health_chronic_filled                 22149 non-null  object        \n",
      " 40  latitude_filled                       20115 non-null  float64       \n",
      " 41  longitude_filled                      20115 non-null  float64       \n",
      " 42  smoking_status_filled                 22120 non-null  object        \n",
      " 43  travel_purpose_filled                 22149 non-null  object        \n",
      " 44  trip_number_filled                    22149 non-null  object        \n",
      " 45  clouds_filled                         22149 non-null  float64       \n",
      " 46  survey_latitude_filled                22149 non-null  float64       \n",
      " 47  survey_longitude_filled               22149 non-null  float64       \n",
      " 48  dew_point_filled                      22149 non-null  float64       \n",
      " 49  feels_like_filled                     22149 non-null  float64       \n",
      " 50  humidity_filled                       22149 non-null  float64       \n",
      " 51  pressure_filled                       22149 non-null  float64       \n",
      " 52  rain_1h_filled                        22149 non-null  float64       \n",
      " 53  snow_1h_filled                        22149 non-null  float64       \n",
      " 54  sunrise_filled                        22149 non-null  float64       \n",
      " 55  sunset_filled                         22149 non-null  float64       \n",
      " 56  temp_filled                           22149 non-null  float64       \n",
      " 57  uvi_filled                            22149 non-null  float64       \n",
      " 58  visibility_filled                     22149 non-null  float64       \n",
      " 59  main_weather_filled                   22149 non-null  object        \n",
      " 60  description_weather_filled            22149 non-null  object        \n",
      " 61  air_quality_components_co_filled      22149 non-null  float64       \n",
      " 62  air_quality_components_nh_3_filled    22149 non-null  float64       \n",
      " 63  air_quality_components_no_filled      22149 non-null  float64       \n",
      " 64  air_quality_components_no_2_filled    22149 non-null  float64       \n",
      " 65  air_quality_components_o_3_filled     22149 non-null  float64       \n",
      " 66  air_quality_components_pm_10_filled   22149 non-null  float64       \n",
      " 67  air_quality_components_pm_2_5_filled  22149 non-null  float64       \n",
      " 68  air_quality_components_so_2_filled    22149 non-null  float64       \n",
      " 69  wind_deg_filled                       22149 non-null  float64       \n",
      " 70  wind_gust_filled                      22149 non-null  float64       \n",
      " 71  wind_speed_filled                     22149 non-null  float64       \n",
      " 72  air_quality_main_filled               22149 non-null  object        \n",
      "dtypes: datetime64[ns](3), float64(29), object(41)\n",
      "memory usage: 12.5+ MB\n"
     ]
    }
   ],
   "source": [
    "itit_filled_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
